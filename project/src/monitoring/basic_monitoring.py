"""
åŸºç¡€ç›‘æ§ç³»ç»Ÿ
===========

å®ç°å®Œæ•´çš„ç›‘æ§ç³»ç»Ÿï¼ŒåŒ…æ‹¬æŒ‡æ ‡æ”¶é›†ã€å¼‚å¸¸æ£€æµ‹ã€å‘Šè­¦ç®¡ç†å’Œå¯è§†åŒ–ç•Œé¢ã€‚
"""

try:
    import torch
    import torch.nn as nn
    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False

try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False

try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False

try:
    import matplotlib.pyplot as plt
    import matplotlib.animation as animation
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False

try:
    import threading
    import time
    import queue
    import json
    import logging
    from typing import Dict, List, Any, Optional, Tuple, Union
    from dataclasses import dataclass, field
    from enum import Enum
    import weakref
    import traceback
    import os
    import sys
except ImportError as e:
    print(f"Missing required dependency: {e}")
    sys.exit(1)


class MetricType(Enum):
    """æŒ‡æ ‡ç±»å‹æšä¸¾"""
    COUNTER = "counter"      # è®¡æ•°å™¨
    GAUGE = "gauge"          # ä»ªè¡¨ç›˜
    HISTOGRAM = "histogram"  # ç›´æ–¹å›¾
    SUMMARY = "summary"      # æ‘˜è¦


class AlertLevel(Enum):
    """å‘Šè­¦çº§åˆ«æšä¸¾"""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


@dataclass
class MetricData:
    """æŒ‡æ ‡æ•°æ®"""
    name: str
    value: float
    timestamp: float
    tags: Dict[str, str] = field(default_factory=dict)
    metric_type: MetricType = MetricType.GAUGE


@dataclass
class Alert:
    """å‘Šè­¦ä¿¡æ¯"""
    id: str
    name: str
    level: AlertLevel
    message: str
    timestamp: float
    metric_name: str
    current_value: float
    threshold: float
    tags: Dict[str, str] = field(default_factory=dict)
    resolved: bool = False
    resolved_timestamp: Optional[float] = None


class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self, max_history_size: int = 10000):
        self.metrics_history: Dict[str, List[MetricData]] = {}
        self.max_history_size = max_history_size
        self.lock = threading.Lock()
        self.is_collecting = False
        self.collection_thread = None
        self.collection_interval = 1.0  # 1ç§’
        self.custom_collectors = {}
        
        # åˆå§‹åŒ–ç³»ç»ŸæŒ‡æ ‡
        self._init_system_metrics()
    
    def _init_system_metrics(self):
        """åˆå§‹åŒ–ç³»ç»ŸæŒ‡æ ‡"""
        self.system_metrics = {
            'cpu_usage': 0.0,
            'memory_usage': 0.0,
            'gpu_usage': 0.0,
            'gpu_memory_usage': 0.0,
            'disk_usage': 0.0,
            'network_io': {'bytes_sent': 0, 'bytes_recv': 0},
            'process_count': 0
        }
    
    def add_metric(self, name: str, value: float, metric_type: MetricType = MetricType.GAUGE,
                   tags: Optional[Dict[str, str]] = None):
        """æ·»åŠ æŒ‡æ ‡"""
        metric = MetricData(
            name=name,
            value=value,
            timestamp=time.time(),
            tags=tags or {},
            metric_type=metric_type
        )
        
        with self.lock:
            if name not in self.metrics_history:
                self.metrics_history[name] = []
            
            self.metrics_history[name].append(metric)
            
            # é™åˆ¶å†å²æ•°æ®å¤§å°
            if len(self.metrics_history[name]) > self.max_history_size:
                self.metrics_history[name] = self.metrics_history[name][-self.max_history_size:]
    
    def get_metric(self, name: str, since: Optional[float] = None) -> List[MetricData]:
        """è·å–æŒ‡æ ‡å†å²æ•°æ®"""
        with self.lock:
            if name not in self.metrics_history:
                return []
            
            history = self.metrics_history[name]
            if since is None:
                return history
            
            return [m for m in history if m.timestamp >= since]
    
    def get_latest_metric(self, name: str) -> Optional[MetricData]:
        """è·å–æœ€æ–°æŒ‡æ ‡å€¼"""
        history = self.get_metric(name)
        return history[-1] if history else None
    
    def collect_system_metrics(self):
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        if not HAS_PSUTIL:
            return
        
        try:
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=0.1)
            self.add_metric('system_cpu_usage', cpu_percent, MetricType.GAUGE)
            
            # å†…å­˜ä½¿ç”¨ç‡
            memory = psutil.virtual_memory()
            self.add_metric('system_memory_usage', memory.percent, MetricType.GAUGE)
            self.add_metric('system_memory_used', memory.used, MetricType.GAUGE)
            self.add_metric('system_memory_total', memory.total, MetricType.GAUGE)
            
            # ç£ç›˜ä½¿ç”¨ç‡
            disk = psutil.disk_usage('/')
            self.add_metric('system_disk_usage', disk.percent, MetricType.GAUGE)
            
            # ç½‘ç»œIO
            net_io = psutil.net_io_counters()
            self.add_metric('system_network_bytes_sent', net_io.bytes_sent, MetricType.COUNTER)
            self.add_metric('system_network_bytes_recv', net_io.bytes_recv, MetricType.COUNTER)
            
            # è¿›ç¨‹æ•°
            process_count = len(psutil.pids())
            self.add_metric('system_process_count', process_count, MetricType.GAUGE)
            
            # GPUæŒ‡æ ‡ï¼ˆå¦‚æœæœ‰PyTorchï¼‰
            if HAS_TORCH and torch.cuda.is_available():
                for i in range(torch.cuda.device_count()):
                    gpu_memory = torch.cuda.memory_allocated(i)
                    try:
                        # å°è¯•è·å–æ€»å†…å­˜
                        if hasattr(torch.cuda, 'memory_properties'):
                            gpu_memory_total = torch.cuda.memory_properties(i).total_memory
                        else:
                            # ä½¿ç”¨æ—§çš„API
                            gpu_memory_total = torch.cuda.get_device_properties(i).total_memory
                        gpu_usage = gpu_memory / gpu_memory_total * 100
                        
                        self.add_metric(f'gpu_{i}_memory_usage', gpu_usage, MetricType.GAUGE,
                                      tags={'device': f'cuda:{i}'})
                        self.add_metric(f'gpu_{i}_memory_allocated', gpu_memory, MetricType.GAUGE,
                                      tags={'device': f'cuda:{i}'})
                        self.add_metric(f'gpu_{i}_memory_total', gpu_memory_total, MetricType.GAUGE,
                                      tags={'device': f'cuda:{i}'})
                    except Exception as gpu_e:
                        logging.warning(f"Error collecting GPU metrics for device {i}: {gpu_e}")
        
        except Exception as e:
            logging.error(f"Error collecting system metrics: {e}")
    
    def collect_training_metrics(self, model, optimizer, loss, batch_size, 
                               learning_rate: Optional[float] = None):
        """æ”¶é›†è®­ç»ƒæŒ‡æ ‡"""
        if HAS_TORCH and hasattr(loss, 'item'):
            # æŸå¤±å€¼
            self.add_metric('training_loss', loss.item(), MetricType.GAUGE)
            
            # å­¦ä¹ ç‡
            if learning_rate is None and optimizer is not None:
                for param_group in optimizer.param_groups:
                    lr = param_group.get('lr', 0.0)
                    self.add_metric('training_learning_rate', lr, MetricType.GAUGE,
                                  tags={'param_group': param_group.get('name', 'default')})
            else:
                self.add_metric('training_learning_rate', learning_rate or 0.0, MetricType.GAUGE)
            
            # æ¢¯åº¦ç»Ÿè®¡
            if optimizer is not None:
                total_norm = 0.0
                for param in model.parameters():
                    if param.grad is not None:
                        param_norm = param.grad.data.norm(2)
                        total_norm += param_norm.item() ** 2
                total_norm = total_norm ** 0.5
                self.add_metric('training_gradient_norm', total_norm, MetricType.GAUGE)
            
            # å‚æ•°æ•°é‡
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            self.add_metric('model_total_parameters', total_params, MetricType.GAUGE)
            self.add_metric('model_trainable_parameters', trainable_params, MetricType.GAUGE)
            
            # æ‰¹æ¬¡å¤§å°
            self.add_metric('training_batch_size', batch_size, MetricType.GAUGE)
        
        else:
            # ç®€åŒ–ç‰ˆæœ¬
            self.add_metric('training_loss', float(loss), MetricType.GAUGE)
            self.add_metric('training_learning_rate', learning_rate or 0.0, MetricType.GAUGE)
            self.add_metric('training_batch_size', batch_size, MetricType.GAUGE)
    
    def add_custom_collector(self, name: str, collector_func):
        """æ·»åŠ è‡ªå®šä¹‰æ”¶é›†å™¨"""
        self.custom_collectors[name] = collector_func
    
    def start_collection(self, interval: float = 1.0):
        """å¼€å§‹æ”¶é›†æŒ‡æ ‡"""
        self.collection_interval = interval
        self.is_collecting = True
        
        if self.collection_thread is None or not self.collection_thread.is_alive():
            self.collection_thread = threading.Thread(target=self._collection_loop)
            self.collection_thread.daemon = True
            self.collection_thread.start()
    
    def stop_collection(self):
        """åœæ­¢æ”¶é›†æŒ‡æ ‡"""
        self.is_collecting = False
        if self.collection_thread:
            self.collection_thread.join(timeout=5.0)
    
    def _collection_loop(self):
        """æ”¶é›†å¾ªç¯"""
        while self.is_collecting:
            try:
                # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
                self.collect_system_metrics()
                
                # æ”¶é›†è‡ªå®šä¹‰æŒ‡æ ‡
                for name, collector in self.custom_collectors.items():
                    try:
                        collector(self)
                    except Exception as e:
                        logging.error(f"Error in custom collector {name}: {e}")
                
                time.sleep(self.collection_interval)
            
            except Exception as e:
                logging.error(f"Error in collection loop: {e}")
                time.sleep(self.collection_interval)
    
    def get_metrics_summary(self) -> Dict[str, Any]:
        """è·å–æŒ‡æ ‡æ‘˜è¦"""
        summary = {}
        
        with self.lock:
            for name, history in self.metrics_history.items():
                if history:
                    latest = history[-1]
                    summary[name] = {
                        'latest_value': latest.value,
                        'timestamp': latest.timestamp,
                        'count': len(history),
                        'metric_type': latest.metric_type.value
                    }
        
        return summary
    
    def export_metrics(self, filename: str, format: str = 'json'):
        """å¯¼å‡ºæŒ‡æ ‡æ•°æ®"""
        if format == 'json':
            data = {}
            with self.lock:
                for name, history in self.metrics_history.items():
                    data[name] = [
                        {
                            'value': m.value,
                            'timestamp': m.timestamp,
                            'tags': m.tags,
                            'type': m.metric_type.value
                        }
                        for m in history
                    ]
            
            with open(filename, 'w') as f:
                json.dump(data, f, indent=2)
        
        elif format == 'csv':
            import csv
            
            with open(filename, 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(['name', 'value', 'timestamp', 'tags', 'type'])
                
                with self.lock:
                    for name, history in self.metrics_history.items():
                        for m in history:
                            writer.writerow([
                                name, m.value, m.timestamp, 
                                json.dumps(m.tags), m.metric_type.value
                            ])


class AnomalyDetector:
    """å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self, window_size: int = 100, sensitivity: float = 2.0):
        self.window_size = window_size
        self.sensitivity = sensitivity  # æ ‡å‡†å·®å€æ•°
        self.baseline_stats: Dict[str, Dict[str, float]] = {}
        self.anomaly_history: List[Dict[str, Any]] = []
        self.lock = threading.Lock()
        
        # å¼‚å¸¸æ£€æµ‹ç®—æ³•
        self.algorithms = {
            'z_score': self._z_score_detection,
            'iqr': self._iqr_detection,
            'moving_average': self._moving_average_detection,
            'isolation_forest': self._isolation_forest_detection
        }
    
    def detect_anomalies(self, metrics_collector: MetricsCollector, 
                        metric_names: Optional[List[str]] = None) -> List[Dict[str, Any]]:
        """æ£€æµ‹å¼‚å¸¸"""
        anomalies = []
        
        if metric_names is None:
            metric_names = list(metrics_collector.metrics_history.keys())
        
        for metric_name in metric_names:
            history = metrics_collector.get_metric(metric_name)
            if len(history) < 10:  # éœ€è¦è¶³å¤Ÿçš„æ•°æ®ç‚¹
                continue
            
            # ä½¿ç”¨å¤šç§ç®—æ³•æ£€æµ‹
            for algorithm_name, algorithm_func in self.algorithms.items():
                try:
                    anomaly_indices = algorithm_func(history)
                    for idx in anomaly_indices:
                        anomaly = {
                            'metric_name': metric_name,
                            'timestamp': history[idx].timestamp,
                            'value': history[idx].value,
                            'algorithm': algorithm_name,
                            'severity': self._calculate_severity(history, idx),
                            'tags': history[idx].tags
                        }
                        anomalies.append(anomaly)
                except Exception as e:
                    logging.error(f"Error in anomaly detection algorithm {algorithm_name}: {e}")
        
        # è®°å½•å¼‚å¸¸å†å²
        with self.lock:
            self.anomaly_history.extend(anomalies)
            # é™åˆ¶å†å²è®°å½•å¤§å°
            if len(self.anomaly_history) > 1000:
                self.anomaly_history = self.anomaly_history[-1000:]
        
        return anomalies
    
    def _z_score_detection(self, history: List[MetricData]) -> List[int]:
        """Z-Scoreå¼‚å¸¸æ£€æµ‹"""
        if len(history) < 2:
            return []
        
        values = [m.value for m in history]
        mean = np.mean(values) if HAS_NUMPY else sum(values) / len(values)
        std = np.std(values) if HAS_NUMPY else (sum((x - mean) ** 2 for x in values) / len(values)) ** 0.5
        
        if std == 0:
            return []
        
        anomalies = []
        for i, value in enumerate(values):
            z_score = abs(value - mean) / std
            if z_score > self.sensitivity:
                anomalies.append(i)
        
        return anomalies
    
    def _iqr_detection(self, history: List[MetricData]) -> List[int]:
        """IQRï¼ˆå››åˆ†ä½è·ï¼‰å¼‚å¸¸æ£€æµ‹"""
        if len(history) < 4:
            return []
        
        values = sorted([m.value for m in history])
        q1 = values[len(values) // 4]
        q3 = values[3 * len(values) // 4]
        iqr = q3 - q1
        
        if iqr == 0:
            return []
        
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        
        anomalies = []
        for i, metric in enumerate(history):
            if metric.value < lower_bound or metric.value > upper_bound:
                anomalies.append(i)
        
        return anomalies
    
    def _moving_average_detection(self, history: List[MetricData]) -> List[int]:
        """ç§»åŠ¨å¹³å‡å¼‚å¸¸æ£€æµ‹"""
        if len(history) < self.window_size:
            return []
        
        anomalies = []
        window_size = min(self.window_size, len(history) // 2)
        
        for i in range(window_size, len(history)):
            window_values = [m.value for m in history[i-window_size:i]]
            moving_avg = sum(window_values) / len(window_values)
            current_value = history[i].value
            
            # è®¡ç®—åå·®
            deviation = abs(current_value - moving_avg) / moving_avg if moving_avg != 0 else 0
            
            if deviation > 0.3:  # 30%åå·®é˜ˆå€¼
                anomalies.append(i)
        
        return anomalies
    
    def _isolation_forest_detection(self, history: List[MetricData]) -> List[int]:
        """å­¤ç«‹æ£®æ—å¼‚å¸¸æ£€æµ‹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        if len(history) < 20:
            return []
        
        # ç®€åŒ–çš„å­¤ç«‹æ£®æ—å®ç°
        values = np.array([[m.value] for m in history]) if HAS_NUMPY else [[m.value] for m in history]
        
        # ä½¿ç”¨ç®€å•çš„ç»Ÿè®¡æ–¹æ³•ä½œä¸ºæ›¿ä»£
        if HAS_NUMPY:
            mean = np.mean(values)
            std = np.std(values)
            z_scores = np.abs((values - mean) / std)
            
            anomalies = []
            for i, z_score in enumerate(z_scores):
                if z_score > self.sensitivity:
                    anomalies.append(i)
            
            return anomalies
        else:
            return self._z_score_detection(history)
    
    def _calculate_severity(self, history: List[MetricData], index: int) -> str:
        """è®¡ç®—å¼‚å¸¸ä¸¥é‡ç¨‹åº¦"""
        if index == 0:
            return "low"
        
        # è®¡ç®—ç›¸å¯¹äºå‰ä¸€ä¸ªç‚¹çš„å˜åŒ–
        change = abs(history[index].value - history[index-1].value)
        relative_change = change / abs(history[index-1].value) if history[index-1].value != 0 else 0
        
        if relative_change > 1.0:  # 100%å˜åŒ–
            return "high"
        elif relative_change > 0.5:  # 50%å˜åŒ–
            return "medium"
        else:
            return "low"
    
    def update_baseline(self, metrics_collector: MetricsCollector, metric_names: Optional[List[str]] = None):
        """æ›´æ–°åŸºçº¿ç»Ÿè®¡"""
        if metric_names is None:
            metric_names = list(metrics_collector.metrics_history.keys())
        
        for metric_name in metric_names:
            history = metrics_collector.get_metric(metric_name)
            if len(history) < 10:
                continue
            
            values = [m.value for m in history]
            
            with self.lock:
                self.baseline_stats[metric_name] = {
                    'mean': np.mean(values) if HAS_NUMPY else sum(values) / len(values),
                    'std': np.std(values) if HAS_NUMPY else (sum((x - sum(values)/len(values)) ** 2 for x in values) / len(values)) ** 0.5,
                    'min': min(values),
                    'max': max(values),
                    'median': np.median(values) if HAS_NUMPY else sorted(values)[len(values)//2],
                    'count': len(values),
                    'last_updated': time.time()
                }
    
    def get_anomaly_summary(self) -> Dict[str, Any]:
        """è·å–å¼‚å¸¸æ‘˜è¦"""
        with self.lock:
            if not self.anomaly_history:
                return {'total_anomalies': 0, 'recent_anomalies': []}
            
            # æœ€è¿‘24å°æ—¶çš„å¼‚å¸¸
            recent_threshold = time.time() - 24 * 3600
            recent_anomalies = [
                a for a in self.anomaly_history 
                if a['timestamp'] > recent_threshold
            ]
            
            # æŒ‰æŒ‡æ ‡åˆ†ç»„
            anomalies_by_metric = {}
            for anomaly in recent_anomalies:
                metric_name = anomaly['metric_name']
                if metric_name not in anomalies_by_metric:
                    anomalies_by_metric[metric_name] = []
                anomalies_by_metric[metric_name].append(anomaly)
            
            return {
                'total_anomalies': len(self.anomaly_history),
                'recent_anomalies': len(recent_anomalies),
                'anomalies_by_metric': anomalies_by_metric,
                'baseline_stats': self.baseline_stats
            }


class AlertManager:
    """å‘Šè­¦ç®¡ç†å™¨"""
    
    def __init__(self):
        self.alerts: Dict[str, Alert] = {}
        self.alert_rules: Dict[str, Dict[str, Any]] = {}
        self.alert_history: List[Alert] = []
        self.notification_handlers: Dict[str, callable] = {}
        self.lock = threading.Lock()
        
        # é»˜è®¤å‘Šè­¦è§„åˆ™
        self._init_default_rules()
    
    def _init_default_rules(self):
        """åˆå§‹åŒ–é»˜è®¤å‘Šè­¦è§„åˆ™"""
        default_rules = {
            'high_cpu_usage': {
                'metric_name': 'system_cpu_usage',
                'condition': 'greater_than',
                'threshold': 80.0,
                'level': AlertLevel.WARNING,
                'duration': 300,  # 5åˆ†é’Ÿ
                'message': 'CPUä½¿ç”¨ç‡è¿‡é«˜: {value}%'
            },
            'high_memory_usage': {
                'metric_name': 'system_memory_usage',
                'condition': 'greater_than',
                'threshold': 85.0,
                'level': AlertLevel.WARNING,
                'duration': 300,
                'message': 'å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {value}%'
            },
            'high_gpu_memory_usage': {
                'metric_name': 'gpu_memory_usage',
                'condition': 'greater_than',
                'threshold': 90.0,
                'level': AlertLevel.WARNING,
                'duration': 60,
                'message': 'GPUå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {value}%'
            },
            'training_loss_spike': {
                'metric_name': 'training_loss',
                'condition': 'spike',
                'threshold': 2.0,  # 2å€å˜åŒ–
                'level': AlertLevel.INFO,
                'duration': 0,
                'message': 'è®­ç»ƒæŸå¤±å‡ºç°å¼‚å¸¸æ³¢åŠ¨: {value}'
            },
            'gradient_explosion': {
                'metric_name': 'training_gradient_norm',
                'condition': 'greater_than',
                'threshold': 10.0,
                'level': AlertLevel.ERROR,
                'duration': 0,
                'message': 'æ¢¯åº¦çˆ†ç‚¸é£é™©: {value}'
            }
        }
        
        for rule_name, rule in default_rules.items():
            self.add_alert_rule(rule_name, rule)
    
    def add_alert_rule(self, name: str, rule: Dict[str, Any]):
        """æ·»åŠ å‘Šè­¦è§„åˆ™"""
        required_fields = ['metric_name', 'condition', 'threshold', 'level']
        for field in required_fields:
            if field not in rule:
                raise ValueError(f"Missing required field: {field}")
        
        self.alert_rules[name] = rule
    
    def remove_alert_rule(self, name: str):
        """ç§»é™¤å‘Šè­¦è§„åˆ™"""
        if name in self.alert_rules:
            del self.alert_rules[name]
    
    def check_alerts(self, metrics_collector: MetricsCollector) -> List[Alert]:
        """æ£€æŸ¥å‘Šè­¦"""
        new_alerts = []
        
        for rule_name, rule in self.alert_rules.items():
            try:
                alert = self._evaluate_rule(rule_name, rule, metrics_collector)
                if alert:
                    new_alerts.append(alert)
            except Exception as e:
                logging.error(f"Error evaluating alert rule {rule_name}: {e}")
        
        # å¤„ç†æ–°å‘Šè­¦
        for alert in new_alerts:
            self._handle_alert(alert)
        
        return new_alerts
    
    def _evaluate_rule(self, rule_name: str, rule: Dict[str, Any], 
                      metrics_collector: MetricsCollector) -> Optional[Alert]:
        """è¯„ä¼°å‘Šè­¦è§„åˆ™"""
        metric_name = rule['metric_name']
        condition = rule['condition']
        threshold = rule['threshold']
        level = rule['level']
        
        # è·å–æŒ‡æ ‡æ•°æ®
        latest_metric = metrics_collector.get_latest_metric(metric_name)
        if not latest_metric:
            return None
        
        current_value = latest_metric.value
        
        # æ£€æŸ¥æ¡ä»¶
        triggered = False
        
        if condition == 'greater_than':
            triggered = current_value > threshold
        elif condition == 'less_than':
            triggered = current_value < threshold
        elif condition == 'equal':
            triggered = abs(current_value - threshold) < 1e-6
        elif condition == 'spike':
            # æ£€æŸ¥æ˜¯å¦å‡ºç°çªå˜
            history = metrics_collector.get_metric(metric_name, 
                                                  time.time() - rule.get('duration', 300))
            if len(history) >= 2:
                prev_value = history[-2].value
                if prev_value != 0:
                    change_ratio = abs(current_value - prev_value) / abs(prev_value)
                    triggered = change_ratio > threshold
        else:
            logging.warning(f"Unknown condition: {condition}")
            return None
        
        if not triggered:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è§£å†³ç°æœ‰å‘Šè­¦
            self._resolve_alert_if_needed(rule_name, current_value)
            return None
        
        # æ£€æŸ¥æŒç»­æ—¶é—´
        duration = rule.get('duration', 0)
        if duration > 0:
            # æ£€æŸ¥æ˜¯å¦æŒç»­è¶…è¿‡æŒ‡å®šæ—¶é—´
            since_time = time.time() - duration
            recent_metrics = metrics_collector.get_metric(metric_name, since_time)
            
            if not recent_metrics:
                return None
            
            # æ£€æŸ¥æ‰€æœ‰æœ€è¿‘çš„æŒ‡æ ‡æ˜¯å¦éƒ½æ»¡è¶³æ¡ä»¶
            all_triggered = True
            for metric in recent_metrics:
                if condition == 'greater_than':
                    if metric.value <= threshold:
                        all_triggered = False
                        break
                elif condition == 'less_than':
                    if metric.value >= threshold:
                        all_triggered = False
                        break
            
            if not all_triggered:
                return None
        
        # ç”Ÿæˆå‘Šè­¦ID
        alert_id = f"{rule_name}_{int(time.time())}"
        
        # ç”Ÿæˆå‘Šè­¦æ¶ˆæ¯
        message = rule['message'].format(value=current_value)
        
        # åˆ›å»ºå‘Šè­¦
        alert = Alert(
            id=alert_id,
            name=rule_name,
            level=level,
            message=message,
            timestamp=time.time(),
            metric_name=metric_name,
            current_value=current_value,
            threshold=threshold,
            tags=latest_metric.tags
        )
        
        return alert
    
    def _resolve_alert_if_needed(self, rule_name: str, current_value: float):
        """æ ¹æ®éœ€è¦è§£å†³å‘Šè­¦"""
        # æŸ¥æ‰¾è¯¥è§„åˆ™çš„æ´»åŠ¨å‘Šè­¦
        active_alerts = [a for a in self.alerts.values() 
                        if a.name == rule_name and not a.resolved]
        
        for alert in active_alerts:
            rule = self.alert_rules.get(rule_name)
            if not rule:
                continue
            
            condition = rule['condition']
            threshold = rule['threshold']
            
            # æ£€æŸ¥æ˜¯å¦æ¢å¤æ­£å¸¸
            resolved = False
            
            if condition == 'greater_than':
                resolved = current_value <= threshold * 0.9  # 10%ç¼“å†²
            elif condition == 'less_than':
                resolved = current_value >= threshold * 1.1  # 10%ç¼“å†²
            elif condition == 'spike':
                resolved = True  # çªå˜å‘Šè­¦è‡ªåŠ¨è§£å†³
            
            if resolved:
                self.resolve_alert(alert.id)
    
    def _handle_alert(self, alert: Alert):
        """å¤„ç†å‘Šè­¦"""
        with self.lock:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒè§„åˆ™çš„æœªè§£å†³å‘Šè­¦
            existing_alerts = [a for a in self.alerts.values() 
                             if a.name == alert.name and not a.resolved]
            
            if existing_alerts:
                # æ›´æ–°ç°æœ‰å‘Šè­¦
                existing_alert = existing_alerts[0]
                existing_alert.current_value = alert.current_value
                existing_alert.timestamp = alert.timestamp
                alert = existing_alert
            else:
                # æ·»åŠ æ–°å‘Šè­¦
                self.alerts[alert.id] = alert
            
            # æ·»åŠ åˆ°å†å²è®°å½•
            self.alert_history.append(alert)
            
            # é™åˆ¶å†å²è®°å½•å¤§å°
            if len(self.alert_history) > 1000:
                self.alert_history = self.alert_history[-1000:]
        
        # å‘é€é€šçŸ¥
        self._send_notifications(alert)
    
    def resolve_alert(self, alert_id: str):
        """è§£å†³å‘Šè­¦"""
        with self.lock:
            if alert_id in self.alerts:
                alert = self.alerts[alert_id]
                if not alert.resolved:
                    alert.resolved = True
                    alert.resolved_timestamp = time.time()
                    
                    # å‘é€è§£å†³é€šçŸ¥
                    self._send_resolved_notification(alert)
    
    def add_notification_handler(self, name: str, handler: callable):
        """æ·»åŠ é€šçŸ¥å¤„ç†å™¨"""
        self.notification_handlers[name] = handler
    
    def _send_notifications(self, alert: Alert):
        """å‘é€é€šçŸ¥"""
        for handler_name, handler in self.notification_handlers.items():
            try:
                handler(alert)
            except Exception as e:
                logging.error(f"Error in notification handler {handler_name}: {e}")
    
    def _send_resolved_notification(self, alert: Alert):
        """å‘é€è§£å†³é€šçŸ¥"""
        # åˆ›å»ºè§£å†³é€šçŸ¥
        resolved_alert = Alert(
            id=f"{alert.id}_resolved",
            name=alert.name,
            level=AlertLevel.INFO,
            message=f"å‘Šè­¦å·²è§£å†³: {alert.message}",
            timestamp=time.time(),
            metric_name=alert.metric_name,
            current_value=alert.current_value,
            threshold=alert.threshold,
            tags=alert.tags,
            resolved=True,
            resolved_timestamp=time.time()
        )
        
        self._send_notifications(resolved_alert)
    
    def get_active_alerts(self) -> List[Alert]:
        """è·å–æ´»åŠ¨å‘Šè­¦"""
        with self.lock:
            return [a for a in self.alerts.values() if not a.resolved]
    
    def get_alert_history(self, limit: int = 100) -> List[Alert]:
        """è·å–å‘Šè­¦å†å²"""
        with self.lock:
            return self.alert_history[-limit:]
    
    def get_alert_summary(self) -> Dict[str, Any]:
        """è·å–å‘Šè­¦æ‘˜è¦"""
        with self.lock:
            active_alerts = self.get_active_alerts()
            
            # æŒ‰çº§åˆ«åˆ†ç»„
            alerts_by_level = {}
            for alert in active_alerts:
                level = alert.level.value
                if level not in alerts_by_level:
                    alerts_by_level[level] = []
                alerts_by_level[level].append(alert)
            
            # æŒ‰è§„åˆ™åˆ†ç»„
            alerts_by_rule = {}
            for alert in active_alerts:
                rule = alert.name
                if rule not in alerts_by_rule:
                    alerts_by_rule[rule] = []
                alerts_by_rule[rule].append(alert)
            
            return {
                'total_active_alerts': len(active_alerts),
                'alerts_by_level': alerts_by_level,
                'alerts_by_rule': alerts_by_rule,
                'total_alerts_count': len(self.alert_history)
            }


class BasicVisualization:
    """åŸºç¡€å¯è§†åŒ–ç•Œé¢"""
    
    def __init__(self, metrics_collector: MetricsCollector, 
                 alert_manager: AlertManager, 
                 anomaly_detector: AnomalyDetector):
        self.metrics_collector = metrics_collector
        self.alert_manager = alert_manager
        self.anomaly_detector = anomaly_detector
        self.is_running = False
        self.update_interval = 5.0  # 5ç§’æ›´æ–°ä¸€æ¬¡
        
        # å¯è§†åŒ–é…ç½®
        self.fig = None
        self.axes = None
        self.animation = None
        
        # å›¾è¡¨æ•°æ®
        self.chart_data = {
            'timestamps': [],
            'cpu_usage': [],
            'memory_usage': [],
            'training_loss': [],
            'gpu_usage': []
        }
    
    def start_dashboard(self, update_interval: float = 5.0):
        """å¯åŠ¨ä»ªè¡¨æ¿"""
        if not HAS_MATPLOTLIB:
            print("Matplotlib not available, using console dashboard")
            self.start_console_dashboard(update_interval)
            return
        
        self.update_interval = update_interval
        self.is_running = True
        
        # åˆ›å»ºå›¾è¡¨
        self._create_charts()
        
        # å¯åŠ¨åŠ¨ç”»
        self.animation = animation.FuncAnimation(
            self.fig, self._update_charts, 
            interval=int(update_interval * 1000),
            blit=False
        )
        
        plt.show()
    
    def start_console_dashboard(self, update_interval: float = 5.0):
        """å¯åŠ¨æ§åˆ¶å°ä»ªè¡¨æ¿"""
        self.update_interval = update_interval
        self.is_running = True
        
        try:
            while self.is_running:
                self._update_console_dashboard()
                time.sleep(update_interval)
        except KeyboardInterrupt:
            self.stop_dashboard()
    
    def _create_charts(self):
        """åˆ›å»ºå›¾è¡¨"""
        self.fig, self.axes = plt.subplots(2, 2, figsize=(12, 8))
        self.fig.suptitle('Picotron ç›‘æ§ä»ªè¡¨æ¿', fontsize=16)
        
        # è®¾ç½®å­å›¾æ ‡é¢˜
        self.axes[0, 0].set_title('ç³»ç»Ÿèµ„æºä½¿ç”¨ç‡')
        self.axes[0, 1].set_title('è®­ç»ƒæŸå¤±')
        self.axes[1, 0].set_title('GPUä½¿ç”¨ç‡')
        self.axes[1, 1].set_title('å‘Šè­¦çŠ¶æ€')
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()
    
    def _update_charts(self, frame):
        """æ›´æ–°å›¾è¡¨"""
        try:
            # æ¸…é™¤æ‰€æœ‰å­å›¾
            for ax in self.axes.flat:
                ax.clear()
            
            # æ›´æ–°ç³»ç»Ÿèµ„æºå›¾è¡¨
            self._update_system_chart()
            
            # æ›´æ–°è®­ç»ƒæŸå¤±å›¾è¡¨
            self._update_training_chart()
            
            # æ›´æ–°GPUä½¿ç”¨ç‡å›¾è¡¨
            self._update_gpu_chart()
            
            # æ›´æ–°å‘Šè­¦çŠ¶æ€å›¾è¡¨
            self._update_alert_chart()
            
            # é‡æ–°è®¾ç½®æ ‡é¢˜
            self.axes[0, 0].set_title('ç³»ç»Ÿèµ„æºä½¿ç”¨ç‡')
            self.axes[0, 1].set_title('è®­ç»ƒæŸå¤±')
            self.axes[1, 0].set_title('GPUä½¿ç”¨ç‡')
            self.axes[1, 1].set_title('å‘Šè­¦çŠ¶æ€')
            
        except Exception as e:
            logging.error(f"Error updating charts: {e}")
    
    def _update_system_chart(self):
        """æ›´æ–°ç³»ç»Ÿèµ„æºå›¾è¡¨"""
        ax = self.axes[0, 0]
        
        # è·å–æœ€è¿‘çš„CPUå’Œå†…å­˜ä½¿ç”¨ç‡
        since_time = time.time() - 300  # 5åˆ†é’Ÿ
        cpu_data = self.metrics_collector.get_metric('system_cpu_usage', since_time)
        memory_data = self.metrics_collector.get_metric('system_memory_usage', since_time)
        
        if cpu_data and memory_data:
            timestamps = [m.timestamp for m in cpu_data]
            cpu_values = [m.value for m in cpu_data]
            memory_values = [m.value for m in memory_data]
            
            ax.plot(timestamps, cpu_values, 'b-', label='CPUä½¿ç”¨ç‡')
            ax.plot(timestamps, memory_values, 'r-', label='å†…å­˜ä½¿ç”¨ç‡')
            ax.set_ylim(0, 100)
            ax.legend()
            ax.set_ylabel('ä½¿ç”¨ç‡ (%)')
            ax.grid(True, alpha=0.3)
    
    def _update_training_chart(self):
        """æ›´æ–°è®­ç»ƒæŸå¤±å›¾è¡¨"""
        ax = self.axes[0, 1]
        
        # è·å–æœ€è¿‘çš„è®­ç»ƒæŸå¤±
        since_time = time.time() - 300  # 5åˆ†é’Ÿ
        loss_data = self.metrics_collector.get_metric('training_loss', since_time)
        
        if loss_data:
            timestamps = [m.timestamp for m in loss_data]
            loss_values = [m.value for m in loss_data]
            
            ax.plot(timestamps, loss_values, 'g-', label='è®­ç»ƒæŸå¤±')
            ax.set_ylabel('æŸå¤±å€¼')
            ax.legend()
            ax.grid(True, alpha=0.3)
    
    def _update_gpu_chart(self):
        """æ›´æ–°GPUä½¿ç”¨ç‡å›¾è¡¨"""
        ax = self.axes[1, 0]
        
        # è·å–GPUæŒ‡æ ‡
        gpu_metrics = []
        for name in self.metrics_collector.metrics_history.keys():
            if name.startswith('gpu_') and name.endswith('_memory_usage'):
                gpu_metrics.append(name)
        
        if gpu_metrics:
            since_time = time.time() - 300  # 5åˆ†é’Ÿ
            
            for metric_name in gpu_metrics:
                data = self.metrics_collector.get_metric(metric_name, since_time)
                if data:
                    timestamps = [m.timestamp for m in data]
                    values = [m.value for m in data]
                    label = metric_name.replace('_memory_usage', '')
                    ax.plot(timestamps, values, label=label)
            
            ax.set_ylim(0, 100)
            ax.set_ylabel('ä½¿ç”¨ç‡ (%)')
            ax.legend()
            ax.grid(True, alpha=0.3)
    
    def _update_alert_chart(self):
        """æ›´æ–°å‘Šè­¦çŠ¶æ€å›¾è¡¨"""
        ax = self.axes[1, 1]
        
        # è·å–å‘Šè­¦æ‘˜è¦
        alert_summary = self.alert_manager.get_alert_summary()
        
        # æ˜¾ç¤ºå‘Šè­¦ç»Ÿè®¡
        active_alerts = alert_summary['total_active_alerts']
        alerts_by_level = alert_summary['alerts_by_level']
        
        # åˆ›å»ºæ¡å½¢å›¾
        levels = list(alerts_by_level.keys())
        counts = [len(alerts_by_level[level]) for level in levels]
        
        if levels:
            colors = ['green', 'yellow', 'orange', 'red']
            bars = ax.bar(levels, counts, color=colors[:len(levels)])
            ax.set_ylabel('å‘Šè­¦æ•°é‡')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, count in zip(bars, counts):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                       str(count), ha='center', va='bottom')
        
        ax.set_title(f'æ´»åŠ¨å‘Šè­¦: {active_alerts}')
    
    def _update_console_dashboard(self):
        """æ›´æ–°æ§åˆ¶å°ä»ªè¡¨æ¿"""
        # æ¸…å±
        os.system('clear' if os.name == 'posix' else 'cls')
        
        print("=" * 60)
        print("Picotron ç›‘æ§ä»ªè¡¨æ¿")
        print("=" * 60)
        print(f"æ›´æ–°æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        # ç³»ç»Ÿèµ„æº
        print("ç³»ç»Ÿèµ„æº:")
        cpu_metric = self.metrics_collector.get_latest_metric('system_cpu_usage')
        memory_metric = self.metrics_collector.get_latest_metric('system_memory_usage')
        
        if cpu_metric:
            print(f"  CPUä½¿ç”¨ç‡: {cpu_metric.value:.1f}%")
        if memory_metric:
            print(f"  å†…å­˜ä½¿ç”¨ç‡: {memory_metric.value:.1f}%")
        
        # è®­ç»ƒæŒ‡æ ‡
        print("\nè®­ç»ƒæŒ‡æ ‡:")
        loss_metric = self.metrics_collector.get_latest_metric('training_loss')
        lr_metric = self.metrics_collector.get_latest_metric('training_learning_rate')
        
        if loss_metric:
            print(f"  è®­ç»ƒæŸå¤±: {loss_metric.value:.4f}")
        if lr_metric:
            print(f"  å­¦ä¹ ç‡: {lr_metric.value:.6f}")
        
        # GPUä½¿ç”¨ç‡
        print("\nGPUä½¿ç”¨ç‡:")
        for name in self.metrics_collector.metrics_history.keys():
            if name.startswith('gpu_') and name.endswith('_memory_usage'):
                metric = self.metrics_collector.get_latest_metric(name)
                if metric:
                    print(f"  {name}: {metric.value:.1f}%")
        
        # å‘Šè­¦çŠ¶æ€
        print("\nå‘Šè­¦çŠ¶æ€:")
        alert_summary = self.alert_manager.get_alert_summary()
        active_alerts = alert_summary['total_active_alerts']
        
        if active_alerts > 0:
            print(f"  æ´»åŠ¨å‘Šè­¦: {active_alerts}")
            for alert in self.alert_manager.get_active_alerts()[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                print(f"    [{alert.level.value.upper()}] {alert.message}")
        else:
            print("  æ— æ´»åŠ¨å‘Šè­¦")
        
        # å¼‚å¸¸æ£€æµ‹
        print("\nå¼‚å¸¸æ£€æµ‹:")
        anomaly_summary = self.anomaly_detector.get_anomaly_summary()
        recent_anomalies = anomaly_summary['recent_anomalies']
        
        if recent_anomalies > 0:
            print(f"  æœ€è¿‘å¼‚å¸¸: {recent_anomalies}")
        else:
            print("  æ— æ£€æµ‹åˆ°å¼‚å¸¸")
        
        print("\næŒ‰ Ctrl+C é€€å‡º...")
    
    def stop_dashboard(self):
        """åœæ­¢ä»ªè¡¨æ¿"""
        self.is_running = False
        if self.animation:
            self.animation.event_source.stop()
        if self.fig:
            plt.close(self.fig)
    
    def generate_report(self, filename: str, time_range: int = 3600):
        """ç”ŸæˆæŠ¥å‘Š"""
        since_time = time.time() - time_range
        
        # æ”¶é›†æ•°æ®
        metrics_summary = self.metrics_collector.get_metrics_summary()
        alert_summary = self.alert_manager.get_alert_summary()
        anomaly_summary = self.anomaly_detector.get_anomaly_summary()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'report_time': time.time(),
            'time_range': time_range,
            'metrics_summary': metrics_summary,
            'alert_summary': alert_summary,
            'anomaly_summary': anomaly_summary,
            'system_info': self._get_system_info()
        }
        
        # ä¿å­˜æŠ¥å‘Š
        with open(filename, 'w') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
    
    def _get_system_info(self):
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        info = {
            'platform': os.name,
            'python_version': sys.version,
            'timestamp': time.time()
        }
        
        if HAS_PSUTIL:
            info.update({
                'cpu_count': psutil.cpu_count(),
                'memory_total': psutil.virtual_memory().total,
                'disk_total': psutil.disk_usage('/').total
            })
        
        if HAS_TORCH and torch.cuda.is_available():
            info.update({
                'cuda_available': True,
                'gpu_count': torch.cuda.device_count(),
                'cuda_version': torch.version.cuda
            })
        
        return info


class BasicMonitoringSystem:
    """åŸºç¡€ç›‘æ§ç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.metrics_collector = MetricsCollector(
            max_history_size=self.config.get('max_history_size', 10000)
        )
        
        self.anomaly_detector = AnomalyDetector(
            window_size=self.config.get('anomaly_window_size', 100),
            sensitivity=self.config.get('anomaly_sensitivity', 2.0)
        )
        
        self.alert_manager = AlertManager()
        
        self.visualization = BasicVisualization(
            self.metrics_collector, self.alert_manager, self.anomaly_detector
        )
        
        # ç›‘æ§çº¿ç¨‹
        self.monitoring_thread = None
        self.is_monitoring = False
        self.monitoring_interval = self.config.get('monitoring_interval', 5.0)
        
        # è®¾ç½®é»˜è®¤é€šçŸ¥å¤„ç†å™¨
        self._setup_default_notifications()
    
    def _setup_default_notifications(self):
        """è®¾ç½®é»˜è®¤é€šçŸ¥å¤„ç†å™¨"""
        # æ§åˆ¶å°é€šçŸ¥
        def console_notification(alert):
            level_icon = {
                AlertLevel.INFO: "â„¹ï¸",
                AlertLevel.WARNING: "âš ï¸",
                AlertLevel.ERROR: "âŒ",
                AlertLevel.CRITICAL: "ğŸš¨"
            }
            
            icon = level_icon.get(alert.level, "ğŸ“¢")
            timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(alert.timestamp))
            
            print(f"{icon} [{alert.level.value.upper()}] {timestamp}")
            print(f"   {alert.message}")
            if alert.resolved:
                print(f"   çŠ¶æ€: å·²è§£å†³")
            print()
        
        self.alert_manager.add_notification_handler('console', console_notification)
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.is_monitoring = True
        
        # å¯åŠ¨æŒ‡æ ‡æ”¶é›†
        self.metrics_collector.start_collection(self.monitoring_interval)
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        if self.monitoring_thread is None or not self.monitoring_thread.is_alive():
            self.monitoring_thread = threading.Thread(target=self._monitoring_loop)
            self.monitoring_thread.daemon = True
            self.monitoring_thread.start()
        
        print("ç›‘æ§ç³»ç»Ÿå·²å¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.is_monitoring = False
        
        # åœæ­¢æŒ‡æ ‡æ”¶é›†
        self.metrics_collector.stop_collection()
        
        # ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5.0)
        
        # åœæ­¢å¯è§†åŒ–
        self.visualization.stop_dashboard()
        
        print("ç›‘æ§ç³»ç»Ÿå·²åœæ­¢")
    
    def _monitoring_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.is_monitoring:
            try:
                # æ£€æŸ¥å‘Šè­¦
                new_alerts = self.alert_manager.check_alerts(self.metrics_collector)
                
                # æ£€æµ‹å¼‚å¸¸
                anomalies = self.anomaly_detector.detect_anomalies(self.metrics_collector)
                
                # å®šæœŸæ›´æ–°åŸºçº¿
                if int(time.time()) % 3600 == 0:  # æ¯å°æ—¶æ›´æ–°ä¸€æ¬¡
                    self.anomaly_detector.update_baseline(self.metrics_collector)
                
                time.sleep(self.monitoring_interval)
            
            except Exception as e:
                logging.error(f"Error in monitoring loop: {e}")
                time.sleep(self.monitoring_interval)
    
    def start_dashboard(self, console_only: bool = False):
        """å¯åŠ¨ä»ªè¡¨æ¿"""
        if console_only or not HAS_MATPLOTLIB:
            self.visualization.start_console_dashboard()
        else:
            self.visualization.start_dashboard()
    
    def add_custom_alert_rule(self, name: str, rule: Dict[str, Any]):
        """æ·»åŠ è‡ªå®šä¹‰å‘Šè­¦è§„åˆ™"""
        self.alert_manager.add_alert_rule(name, rule)
    
    def add_custom_metric_collector(self, name: str, collector_func):
        """æ·»åŠ è‡ªå®šä¹‰æŒ‡æ ‡æ”¶é›†å™¨"""
        self.metrics_collector.add_custom_collector(name, collector_func)
    
    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        return {
            'is_monitoring': self.is_monitoring,
            'metrics_summary': self.metrics_collector.get_metrics_summary(),
            'alert_summary': self.alert_manager.get_alert_summary(),
            'anomaly_summary': self.anomaly_detector.get_anomaly_summary(),
            'timestamp': time.time()
        }
    
    def export_data(self, output_dir: str):
        """å¯¼å‡ºç›‘æ§æ•°æ®"""
        import os
        
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # å¯¼å‡ºæŒ‡æ ‡æ•°æ®
        metrics_file = os.path.join(output_dir, 'metrics.json')
        self.metrics_collector.export_metrics(metrics_file)
        
        # ç”ŸæˆæŠ¥å‘Š
        report_file = os.path.join(output_dir, 'monitoring_report.json')
        self.visualization.generate_report(report_file)
        
        print(f"ç›‘æ§æ•°æ®å·²å¯¼å‡ºåˆ°: {output_dir}")


__all__ = [
    'MetricsCollector', 'AnomalyDetector', 'AlertManager', 
    'BasicVisualization', 'BasicMonitoringSystem',
    'MetricType', 'AlertLevel', 'MetricData', 'Alert'
]