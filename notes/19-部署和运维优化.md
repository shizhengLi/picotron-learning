# 部署和运维优化

## 1. 引言

随着Picotron项目的发展，部署和运维的重要性日益凸显。一个优秀的部署和运维体系可以显著提高项目的可用性、可靠性和可维护性。本文档详细规划了Picotron的部署架构、运维策略和优化方案。

## 2. 部署架构设计

### 2.1 容器化部署

#### 2.1.1 Docker容器化

**实现方案：**
```dockerfile
# 基础镜像
FROM pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime

# 设置工作目录
WORKDIR /app

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    git \
    wget \
    curl \
    vim \
    htop \
    && rm -rf /var/lib/apt/lists/*

# 复制requirements文件
COPY requirements.txt .

# 安装Python依赖
RUN pip install --no-cache-dir -r requirements.txt

# 复制项目代码
COPY . .

# 安装项目
RUN pip install -e .

# 设置环境变量
ENV PYTHONPATH=/app
ENV CUDA_VISIBLE_DEVICES=0
ENV NCCL_DEBUG=INFO

# 创建非root用户
RUN useradd -m -u 1000 picotron
USER picotron

# 暴露端口（如果需要）
EXPOSE 8080

# 健康检查
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD python -c "import torch; import picotron; print('OK')"

# 默认命令
CMD ["python", "-m", "picotron.train", "--config", "config/default.yaml"]
```

#### 2.1.2 Docker Compose编排

**实现方案：**
```yaml
# docker-compose.yml
version: '3.8'

services:
  # 主节点
  master:
    build: .
    image: picotron:latest
    command: ["python", "-m", "picotron.launcher", "--role", "master"]
    environment:
      - RANK=0
      - WORLD_SIZE=4
      - MASTER_ADDR=master
      - MASTER_PORT=29500
    ports:
      - "8080:8080"
      - "29500:29500"
    volumes:
      - ./data:/app/data
      - ./checkpoints:/app/checkpoints
      - ./logs:/app/logs
    networks:
      - picotron-net
    depends_on:
      - etcd
      - prometheus
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 16G
        reservations:
          cpus: '2.0'
          memory: 8G

  # 工作节点
  worker:
    build: .
    image: picotron:latest
    command: ["python", "-m", "picotron.launcher", "--role", "worker"]
    environment:
      - RANK
      - WORLD_SIZE=4
      - MASTER_ADDR=master
      - MASTER_PORT=29500
    volumes:
      - ./data:/app/data
      - ./checkpoints:/app/checkpoints
      - ./logs:/app/logs
    networks:
      - picotron-net
    depends_on:
      - master
      - etcd
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '4.0'
          memory: 16G
          nvidia.com/gpu: 1
        reservations:
          cpus: '2.0'
          memory: 8G
          nvidia.com/gpu: 1

  # 数据加载服务
  dataloader:
    build: .
    image: picotron:latest
    command: ["python", "-m", "picotron.dataloader", "--port", "8081"]
    ports:
      - "8081:8081"
    volumes:
      - ./data:/app/data
    networks:
      - picotron-net
    depends_on:
      - etcd

  # 监控服务
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    networks:
      - picotron-net

  # Grafana可视化
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    volumes:
      - ./monitoring/grafana:/var/lib/grafana
      - ./monitoring/grafana-provisioning:/etc/grafana/provisioning
    networks:
      - picotron-net
    depends_on:
      - prometheus

  # 日志收集
  fluentd:
    image: fluent/fluentd:v1.16-1
    volumes:
      - ./logs:/fluentd/log
      - ./monitoring/fluentd/conf:/fluentd/etc
    networks:
      - picotron-net

  # 分布式存储
  minio:
    image: minio/minio:latest
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
    command: server /data --console-address ":9001"
    networks:
      - picotron-net

  # 服务发现
  etcd:
    image: quay.io/coreos/etcd:v3.5.0
    ports:
      - "2379:2379"
      - "2380:2380"
    volumes:
      - etcd_data:/etcd
    environment:
      - ETCDCTL_API=3
      - ETCD_NAME=etcd0
      - ETCD_INITIAL_ADVERTISE_PEER_URLS=http://etcd:2380
      - ETCD_LISTEN_PEER_URLS=http://0.0.0.0:2380
      - ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379
      - ETCD_INITIAL_CLUSTER=etcd0=http://etcd:2380
    networks:
      - picotron-net

networks:
  picotron-net:
    driver: bridge

volumes:
  prometheus_data:
  grafana_data:
  minio_data:
  etcd_data:
```

### 2.2 Kubernetes部署

#### 2.2.1 Kubernetes资源配置

**实现方案：**
```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: picotron
  labels:
    name: picotron

---
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: picotron-config
  namespace: picotron
data:
  config.yaml: |
    training:
      batch_size: 32
      learning_rate: 0.001
      epochs: 100
    
    model:
      name: llama-7b
      hidden_size: 4096
      num_layers: 32
    
    distributed:
      tensor_parallel_size: 2
      pipeline_parallel_size: 2
      data_parallel_size: 2

---
# k8s/master-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: picotron-master
  namespace: picotron
spec:
  replicas: 1
  selector:
    matchLabels:
      app: picotron-master
  template:
    metadata:
      labels:
        app: picotron-master
    spec:
      containers:
      - name: picotron
        image: picotron:latest
        command: ["python", "-m", "picotron.launcher"]
        args: ["--role", "master"]
        env:
        - name: RANK
          value: "0"
        - name: WORLD_SIZE
          value: "4"
        - name: MASTER_ADDR
          value: "picotron-master-0.picotron-master"
        - name: MASTER_PORT
          value: "29500"
        ports:
        - containerPort: 8080
        - containerPort: 29500
        resources:
          requests:
            cpu: "2"
            memory: "8Gi"
            nvidia.com/gpu: "1"
          limits:
            cpu: "4"
            memory: "16Gi"
            nvidia.com/gpu: "1"
        volumeMounts:
        - name: data
          mountPath: /app/data
        - name: checkpoints
          mountPath: /app/checkpoints
        - name: logs
          mountPath: /app/logs
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: picotron-data-pvc
      - name: checkpoints
        persistentVolumeClaim:
          claimName: picotron-checkpoints-pvc
      - name: logs
        persistentVolumeClaim:
          claimName: picotron-logs-pvc

---
# k8s/worker-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: picotron-worker
  namespace: picotron
spec:
  serviceName: picotron-worker
  replicas: 3
  selector:
    matchLabels:
      app: picotron-worker
  template:
    metadata:
      labels:
        app: picotron-worker
    spec:
      containers:
      - name: picotron
        image: picotron:latest
        command: ["python", "-m", "picotron.launcher"]
        args: ["--role", "worker"]
        env:
        - name: RANK
          valueFrom:
            fieldRef:
              fieldPath: metadata.annotations['rank']
        - name: WORLD_SIZE
          value: "4"
        - name: MASTER_ADDR
          value: "picotron-master-0.picotron-master"
        - name: MASTER_PORT
          value: "29500"
        resources:
          requests:
            cpu: "2"
            memory: "8Gi"
            nvidia.com/gpu: "1"
          limits:
            cpu: "4"
            memory: "16Gi"
            nvidia.com/gpu: "1"
        volumeMounts:
        - name: data
          mountPath: /app/data
        - name: checkpoints
          mountPath: /app/checkpoints
        - name: logs
          mountPath: /app/logs
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: picotron-data-pvc
      - name: checkpoints
        persistentVolumeClaim:
          claimName: picotron-checkpoints-pvc
      - name: logs
        persistentVolumeClaim:
          claimName: picotron-logs-pvc

---
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: picotron-master
  namespace: picotron
spec:
  selector:
    app: picotron-master
  ports:
  - name: http
    port: 8080
    targetPort: 8080
  - name: comm
    port: 29500
    targetPort: 29500
  type: ClusterIP

---
# k8s/monitoring/prometheus-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: picotron
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
      - name: prometheus
        image: prom/prometheus:latest
        ports:
        - containerPort: 9090
        volumeMounts:
        - name: prometheus-config
          mountPath: /etc/prometheus
      volumes:
      - name: prometheus-config
        configMap:
          name: prometheus-config

---
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: picotron-worker-hpa
  namespace: picotron
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: picotron-worker
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

#### 2.2.2 Helm Chart部署

**实现方案：**
```yaml
# charts/picotron/Chart.yaml
apiVersion: v2
name: picotron
description: A Helm chart for Picotron distributed training framework
version: 0.1.0
appVersion: "1.0.0"

---
# charts/picotron/values.yaml
# Default values for picotron.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: picotron
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "latest"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 8080

ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources:
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  limits:
    cpu: 4
    memory: 16Gi
    nvidia.com/gpu: 1
  requests:
    cpu: 2
    memory: 8Gi
    nvidia.com/gpu: 1

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}

# Picotron specific configuration
picotron:
  config:
    training:
      batch_size: 32
      learning_rate: 0.001
      epochs: 100
    
    model:
      name: llama-7b
      hidden_size: 4096
      num_layers: 32
    
    distributed:
      tensor_parallel_size: 2
      pipeline_parallel_size: 2
      data_parallel_size: 2

  master:
    enabled: true
    replicas: 1
  
  worker:
    enabled: true
    replicas: 3

  monitoring:
    enabled: true
    prometheus:
      enabled: true
    grafana:
      enabled: true

  storage:
    enabled: true
    minio:
      enabled: true
      accessKey: admin
      secretKey: password
```

## 3. 监控和日志系统

### 3.1 监控系统架构

#### 3.1.1 Prometheus监控

**实现方案：**
```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  # Prometheus自身监控
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Picotron Master监控
  - job_name: 'picotron-master'
    static_configs:
      - targets: ['picotron-master:8080']
    metrics_path: '/metrics'
    scrape_interval: 5s

  # Picotron Worker监控
  - job_name: 'picotron-worker'
    static_configs:
      - targets: ['picotron-worker-0:8080', 'picotron-worker-1:8080', 'picotron-worker-2:8080']
    metrics_path: '/metrics'
    scrape_interval: 5s

  # GPU监控
  - job_name: 'nvidia-gpu'
    static_configs:
      - targets: ['node-exporter:9100']
    metrics_path: '/metrics'
    scrape_interval: 5s

  # 系统监控
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
    metrics_path: '/metrics'
    scrape_interval: 5s

---
# monitoring/alert_rules.yml
groups:
  - name: picotron-alerts
    rules:
      # 高GPU内存使用告警
      - alert: HighGPUMemoryUsage
        expr: gpu_memory_used / gpu_memory_total * 100 > 90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High GPU memory usage detected"
          description: "GPU memory usage is {{ $value }}% on {{ $labels.instance }}"

      # GPU使用率低告警
      - alert: LowGPUUtilization
        expr: gpu_utilization < 20
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Low GPU utilization detected"
          description: "GPU utilization is {{ $value }}% on {{ $labels.instance }}"

      # 训练速度慢告警
      - alert: SlowTrainingSpeed
        expr: training_steps_per_second < 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Slow training speed detected"
          description: "Training speed is {{ $value }} steps/second"

      # 通信延迟高告警
      - alert: HighCommunicationLatency
        expr: communication_latency_ms > 100
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High communication latency detected"
          description: "Communication latency is {{ $value }}ms"

      # 节点离线告警
      - alert: NodeOffline
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Node is offline"
          description: "Node {{ $labels.instance }} is offline"
```

#### 3.1.2 Grafana仪表板

**实现方案：**
```json
{
  "dashboard": {
    "id": null,
    "title": "Picotron Training Dashboard",
    "tags": ["picotron", "training", "distributed"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Training Progress",
        "type": "stat",
        "targets": [
          {
            "expr": "training_total_steps",
            "refId": "A"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "none"
          }
        }
      },
      {
        "id": 2,
        "title": "GPU Utilization",
        "type": "graph",
        "targets": [
          {
            "expr": "gpu_utilization",
            "refId": "A"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent"
          }
        }
      },
      {
        "id": 3,
        "title": "Memory Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "gpu_memory_used_bytes",
            "refId": "A"
          },
          {
            "expr": "gpu_memory_total_bytes",
            "refId": "B"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "bytes"
          }
        }
      },
      {
        "id": 4,
        "title": "Training Speed",
        "type": "graph",
        "targets": [
          {
            "expr": "training_steps_per_second",
            "refId": "A"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "steps/sec"
          }
        }
      },
      {
        "id": 5,
        "title": "Loss",
        "type": "graph",
        "targets": [
          {
            "expr": "training_loss",
            "refId": "A"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "none"
          }
        }
      },
      {
        "id": 6,
        "title": "Communication Latency",
        "type": "graph",
        "targets": [
          {
            "expr": "communication_latency_ms",
            "refId": "A"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "ms"
          }
        }
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "5s"
  }
}
```

### 3.2 日志管理系统

#### 3.2.1 ELK Stack日志收集

**实现方案：**
```yaml
# logging/elasticsearch.yml
cluster.name: picotron-logging
node.name: elasticsearch
network.host: 0.0.0.0
http.port: 9200
discovery.type: single-node

---
# logging/logstash.conf
input {
  beats {
    port => 5044
  }
}

filter {
  if [fields][service] == "picotron" {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:message}" }
    }
    
    date {
      match => [ "timestamp", "ISO8601" ]
    }
    
    if [level] == "ERROR" {
      mutate {
        add_tag => ["error"]
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "picotron-logs-%{+YYYY.MM.dd}"
  }
}

---
# logging/kibana.yml
server.name: kibana
server.host: 0.0.0.0
elasticsearch.hosts: ["http://elasticsearch:9200"]

---
# logging/filebeat.yml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /app/logs/*.log
  fields:
    service: picotron
    environment: production

output.logstash:
  hosts: ["logstash:5044"]

processors:
- add_host_metadata:
    when.not.contains.tags: forwarded
- add_cloud_metadata: ~
- add_docker_metadata: ~
```

#### 3.2.2 结构化日志

**实现方案：**
```python
# picotron/utils/logging.py
import logging
import json
import sys
from datetime import datetime
from typing import Dict, Any, Optional
import structlog

class StructuredLogger:
    """结构化日志记录器"""
    
    def __init__(self, name: str, level: str = "INFO"):
        self.name = name
        self.level = getattr(logging, level.upper())
        
        # 配置structlog
        structlog.configure(
            processors=[
                structlog.stdlib.filter_by_level,
                structlog.stdlib.add_logger_name,
                structlog.stdlib.add_log_level,
                structlog.stdlib.PositionalArgumentsFormatter(),
                structlog.processors.TimeStamper(fmt="iso"),
                structlog.processors.StackInfoRenderer(),
                structlog.processors.format_exc_info,
                structlog.processors.UnicodeDecoder(),
                structlog.processors.JSONRenderer()
            ],
            wrapper_class=structlog.stdlib.BoundLogger,
            logger_factory=structlog.stdlib.LoggerFactory(),
            cache_logger_on_first_use=True,
        )
        
        self.logger = structlog.get_logger(name)
    
    def info(self, message: str, **kwargs):
        """记录INFO级别日志"""
        self.logger.info(message, **kwargs)
    
    def warning(self, message: str, **kwargs):
        """记录WARNING级别日志"""
        self.logger.warning(message, **kwargs)
    
    def error(self, message: str, **kwargs):
        """记录ERROR级别日志"""
        self.logger.error(message, **kwargs)
    
    def debug(self, message: str, **kwargs):
        """记录DEBUG级别日志"""
        self.logger.debug(message, **kwargs)
    
    def critical(self, message: str, **kwargs):
        """记录CRITICAL级别日志"""
        self.logger.critical(message, **kwargs)
    
    def exception(self, message: str, **kwargs):
        """记录异常日志"""
        self.logger.exception(message, **kwargs)
    
    def log_training_event(self, event_type: str, **kwargs):
        """记录训练事件"""
        event_data = {
            'event_type': event_type,
            'timestamp': datetime.utcnow().isoformat(),
            'service': 'picotron-training',
            **kwargs
        }
        
        self.info(f"Training event: {event_type}", **event_data)
    
    def log_performance_metrics(self, metrics: Dict[str, Any]):
        """记录性能指标"""
        self.info("Performance metrics", **metrics)
    
    def log_error_with_context(self, error: Exception, context: Dict[str, Any]):
        """记录带上下文的错误"""
        error_data = {
            'error_type': type(error).__name__,
            'error_message': str(error),
            'context': context,
            'timestamp': datetime.utcnow().isoformat()
        }
        
        self.exception("Error occurred", **error_data)

class TrainingLogger:
    """训练专用日志记录器"""
    
    def __init__(self, experiment_name: str, log_dir: str = "logs"):
        self.experiment_name = experiment_name
        self.log_dir = log_dir
        
        # 创建主日志记录器
        self.main_logger = StructuredLogger(f"picotron.{experiment_name}")
        
        # 创建指标日志记录器
        self.metrics_logger = StructuredLogger(f"picotron.{experiment_name}.metrics")
        
        # 设置文件处理器
        self._setup_file_handlers()
    
    def _setup_file_handlers(self):
        """设置文件处理器"""
        import os
        os.makedirs(self.log_dir, exist_ok=True)
        
        # 主日志文件
        main_log_file = os.path.join(self.log_dir, f"{self.experiment_name}.log")
        
        # 指标日志文件
        metrics_log_file = os.path.join(self.log_dir, f"{self.experiment_name}_metrics.jsonl")
        
        # 这里可以添加文件处理器的配置
        # 由于structlog的配置较为复杂，这里简化处理
    
    def log_step(self, step: int, loss: float, metrics: Dict[str, float]):
        """记录训练步骤"""
        step_data = {
            'step': step,
            'loss': loss,
            'metrics': metrics,
            'timestamp': datetime.utcnow().isoformat()
        }
        
        self.main_logger.info("Training step", **step_data)
        self.metrics_logger.info("Metrics", **step_data)
    
    def log_epoch(self, epoch: int, avg_loss: float, metrics: Dict[str, float]):
        """记录训练周期"""
        epoch_data = {
            'epoch': epoch,
            'avg_loss': avg_loss,
            'metrics': metrics,
            'timestamp': datetime.utcnow().isoformat()
        }
        
        self.main_logger.info("Training epoch", **epoch_data)
    
    def log_checkpoint(self, step: int, checkpoint_path: str, metrics: Dict[str, float]):
        """记录检查点"""
        checkpoint_data = {
            'step': step,
            'checkpoint_path': checkpoint_path,
            'metrics': metrics,
            'timestamp': datetime.utcnow().isoformat()
        }
        
        self.main_logger.info("Checkpoint saved", **checkpoint_data)
    
    def log_evaluation(self, step: int, eval_metrics: Dict[str, float]):
        """记录评估结果"""
        eval_data = {
            'step': step,
            'eval_metrics': eval_metrics,
            'timestamp': datetime.utcnow().isoformat()
        }
        
        self.main_logger.info("Evaluation completed", **eval_data)
    
    def log_system_info(self):
        """记录系统信息"""
        import torch
        import psutil
        import platform
        
        system_info = {
            'python_version': platform.python_version(),
            'pytorch_version': torch.__version__,
            'cuda_version': torch.version.cuda if torch.cuda.is_available() else None,
            'cpu_count': psutil.cpu_count(),
            'memory_total': psutil.virtual_memory().total,
            'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'timestamp': datetime.utcnow().isoformat()
        }
        
        self.main_logger.info("System info", **system_info)
    
    def log_hyperparameters(self, hyperparameters: Dict[str, Any]):
        """记录超参数"""
        hparam_data = {
            'hyperparameters': hyperparameters,
            'timestamp': datetime.utcnow().isoformat()
        }
        
        self.main_logger.info("Hyperparameters", **hparam_data)

# 全局日志记录器实例
_global_logger = None

def get_logger(name: str = "picotron") -> StructuredLogger:
    """获取全局日志记录器"""
    global _global_logger
    if _global_logger is None:
        _global_logger = StructuredLogger(name)
    return _global_logger

def setup_logging(config: Dict[str, Any]):
    """设置日志配置"""
    log_level = config.get('log_level', 'INFO')
    log_format = config.get('log_format', 'json')
    log_file = config.get('log_file', None)
    
    # 创建日志记录器
    logger = get_logger()
    
    # 设置日志级别
    logger.setLevel(log_level)
    
    # 如果需要，添加文件处理器
    if log_file:
        # 这里可以添加文件处理器的配置
        pass
    
    return logger
```

## 4. 自动化运维

### 4.1 CI/CD 流水线

#### 4.1.1 GitHub Actions CI/CD

**实现方案：**
```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Log in to the Container registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy-to-staging:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: staging
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Deploy to staging
      run: |
        # 部署到staging环境
        echo "Deploying to staging environment..."
        # 这里可以添加具体的部署命令
    
    - name: Run smoke tests
      run: |
        # 运行冒烟测试
        echo "Running smoke tests..."
        # 这里可以添加具体的测试命令

  deploy-to-production:
    needs: deploy-to-staging
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Deploy to production
      run: |
        # 部署到production环境
        echo "Deploying to production environment..."
        # 这里可以添加具体的部署命令
    
    - name: Notify deployment
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ job.status }}
        channel: '#deployments'
        webhook_url: ${{ secrets.SLACK_WEBHOOK }}
```

#### 4.1.2 自动化脚本

**实现方案：**
```bash
#!/bin/bash
# scripts/deploy.sh

set -e

# 颜色定义
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# 日志函数
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# 配置变量
ENVIRONMENT=${1:-staging}
CLUSTER_NAME="picotron-${ENVIRONMENT}"
REGION="us-west-2"
NAMESPACE="picotron"

# 检查参数
if [[ ! "$ENVIRONMENT" =~ ^(staging|production)$ ]]; then
    log_error "Invalid environment: $ENVIRONMENT. Must be 'staging' or 'production'"
    exit 1
fi

log_info "Deploying to environment: $ENVIRONMENT"

# 检查依赖
log_info "Checking dependencies..."

# 检查kubectl
if ! command -v kubectl &> /dev/null; then
    log_error "kubectl is not installed"
    exit 1
fi

# 检查helm
if ! command -v helm &> /dev/null; then
    log_error "helm is not installed"
    exit 1
fi

# 检查aws-cli (如果使用AWS)
if ! command -v aws &> /dev/null; then
    log_warn "aws-cli is not installed, skipping AWS-specific checks"
fi

# 更新kubeconfig
log_info "Updating kubeconfig..."
aws eks update-kubeconfig --name $CLUSTER_NAME --region $REGION

# 创建namespace
log_info "Creating namespace..."
kubectl create namespace $NAMESPACE --dry-run=client -o yaml | kubectl apply -f -

# 部署监控组件
log_info "Deploying monitoring components..."
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# 部署Prometheus
helm upgrade --install prometheus prometheus-community/kube-prometheus-stack \
    --namespace monitoring \
    --create-namespace \
    --set grafana.adminPassword=admin123 \
    --wait

# 部署Picotron
log_info "Deploying Picotron..."
helm upgrade --install picotron ./charts/picotron \
    --namespace $NAMESPACE \
    --set environment=$ENVIRONMENT \
    --set monitoring.enabled=true \
    --wait

# 验证部署
log_info "Verifying deployment..."
kubectl get pods -n $NAMESPACE

# 检查Pod状态
log_info "Checking pod status..."
kubectl wait --for=condition=ready pod -l app=picotron -n $NAMESPACE --timeout=300s

# 运行健康检查
log_info "Running health checks..."
./scripts/health-check.sh $ENVIRONMENT

# 输出访问信息
log_info "Deployment completed successfully!"
log_info "Access URLs:"
log_info "- Grafana: http://localhost:3000 (admin/admin123)"
log_info "- Prometheus: http://localhost:9090"
log_info "- Picotron API: http://localhost:8080"

log_info "To view logs:"
log_info "kubectl logs -f deployment/picotron-master -n $NAMESPACE"

log_info "To port forward:"
log_info "kubectl port-forward svc/picotron-master 8080:8080 -n $NAMESPACE"
```

### 4.2 自动化运维工具

#### 4.2.1 自动扩缩容

**实现方案：**
```python
# picotron/ops/auto_scaling.py
import time
import requests
import logging
from typing import Dict, List, Optional
from dataclasses import dataclass
from enum import Enum

class ScalingDirection(Enum):
    UP = "up"
    DOWN = "down"

@dataclass
class ScalingMetric:
    """扩缩容指标"""
    name: str
    value: float
    threshold: float
    comparison: str  # "gt" or "lt"

@dataclass
class ScalingConfig:
    """扩缩容配置"""
    min_replicas: int
    max_replicas: int
    scale_up_threshold: float
    scale_down_threshold: float
    cooldown_period: int  # seconds
    metrics: List[ScalingMetric]

class AutoScaler:
    """自动扩缩容器"""
    
    def __init__(self, config: ScalingConfig):
        self.config = config
        self.last_scale_time = 0
        self.logger = logging.getLogger(__name__)
        
        # Kubernetes API客户端
        self.k8s_client = self._init_k8s_client()
        
        # 监控客户端
        self.prometheus_client = self._init_prometheus_client()
    
    def _init_k8s_client(self):
        """初始化Kubernetes客户端"""
        try:
            from kubernetes import client, config
            
            # 加载kubeconfig
            config.load_kube_config()
            
            # 创建API客户端
            self.apps_v1 = client.AppsV1Api()
            self.core_v1 = client.CoreV1Api()
            
            return True
        except ImportError:
            self.logger.error("kubernetes package not installed")
            return False
        except Exception as e:
            self.logger.error(f"Failed to initialize Kubernetes client: {e}")
            return False
    
    def _init_prometheus_client(self):
        """初始化Prometheus客户端"""
        try:
            self.prometheus_url = "http://prometheus:9090"
            return True
        except Exception as e:
            self.logger.error(f"Failed to initialize Prometheus client: {e}")
            return False
    
    def check_scaling_conditions(self) -> Optional[ScalingDirection]:
        """检查扩缩容条件"""
        if time.time() - self.last_scale_time < self.config.cooldown_period:
            return None
        
        # 获取当前指标
        metrics = self._get_current_metrics()
        
        # 检查扩容条件
        if self._should_scale_up(metrics):
            return ScalingDirection.UP
        
        # 检查缩容条件
        if self._should_scale_down(metrics):
            return ScalingDirection.DOWN
        
        return None
    
    def _get_current_metrics(self) -> Dict[str, float]:
        """获取当前指标"""
        metrics = {}
        
        for metric in self.config.metrics:
            try:
                value = self._query_prometheus(metric.name)
                metrics[metric.name] = value
            except Exception as e:
                self.logger.error(f"Failed to get metric {metric.name}: {e}")
                metrics[metric.name] = 0.0
        
        return metrics
    
    def _query_prometheus(self, query: str) -> float:
        """查询Prometheus"""
        try:
            response = requests.get(
                f"{self.prometheus_url}/api/v1/query",
                params={"query": query}
            )
            
            if response.status_code == 200:
                data = response.json()
                if data['data']['result']:
                    return float(data['data']['result'][0]['value'][1])
            
            return 0.0
        except Exception as e:
            self.logger.error(f"Failed to query Prometheus: {e}")
            return 0.0
    
    def _should_scale_up(self, metrics: Dict[str, float]) -> bool:
        """判断是否需要扩容"""
        for metric in self.config.metrics:
            current_value = metrics.get(metric.name, 0.0)
            
            if metric.comparison == "gt" and current_value > metric.threshold:
                self.logger.info(f"Scale up condition met: {metric.name}={current_value} > {metric.threshold}")
                return True
            elif metric.comparison == "lt" and current_value < metric.threshold:
                self.logger.info(f"Scale up condition met: {metric.name}={current_value} < {metric.threshold}")
                return True
        
        return False
    
    def _should_scale_down(self, metrics: Dict[str, float]) -> bool:
        """判断是否需要缩容"""
        for metric in self.config.metrics:
            current_value = metrics.get(metric.name, 0.0)
            
            if metric.comparison == "gt" and current_value < metric.threshold * 0.8:
                self.logger.info(f"Scale down condition met: {metric.name}={current_value} < {metric.threshold * 0.8}")
                return True
            elif metric.comparison == "lt" and current_value > metric.threshold * 1.2:
                self.logger.info(f"Scale down condition met: {metric.name}={current_value} > {metric.threshold * 1.2}")
                return True
        
        return False
    
    def scale(self, direction: ScalingDirection) -> bool:
        """执行扩缩容"""
        try:
            if direction == ScalingDirection.UP:
                return self._scale_up()
            else:
                return self._scale_down()
        except Exception as e:
            self.logger.error(f"Failed to scale {direction.value}: {e}")
            return False
    
    def _scale_up(self) -> bool:
        """扩容"""
        # 获取当前副本数
        current_replicas = self._get_current_replicas()
        
        if current_replicas >= self.config.max_replicas:
            self.logger.info(f"Already at maximum replicas: {current_replicas}")
            return False
        
        # 计算新副本数
        new_replicas = min(current_replicas + 1, self.config.max_replicas)
        
        # 更新副本数
        if self._update_replicas(new_replicas):
            self.last_scale_time = time.time()
            self.logger.info(f"Scaled up from {current_replicas} to {new_replicas} replicas")
            return True
        
        return False
    
    def _scale_down(self) -> bool:
        """缩容"""
        # 获取当前副本数
        current_replicas = self._get_current_replicas()
        
        if current_replicas <= self.config.min_replicas:
            self.logger.info(f"Already at minimum replicas: {current_replicas}")
            return False
        
        # 计算新副本数
        new_replicas = max(current_replicas - 1, self.config.min_replicas)
        
        # 更新副本数
        if self._update_replicas(new_replicas):
            self.last_scale_time = time.time()
            self.logger.info(f"Scaled down from {current_replicas} to {new_replicas} replicas")
            return True
        
        return False
    
    def _get_current_replicas(self) -> int:
        """获取当前副本数"""
        try:
            deployment = self.apps_v1.read_namespaced_deployment(
                name="picotron-worker",
                namespace="picotron"
            )
            return deployment.spec.replicas
        except Exception as e:
            self.logger.error(f"Failed to get current replicas: {e}")
            return 0
    
    def _update_replicas(self, new_replicas: int) -> bool:
        """更新副本数"""
        try:
            # 更新StatefulSet副本数
            self.apps_v1.patch_namespaced_stateful_set(
                name="picotron-worker",
                namespace="picotron",
                body={"spec": {"replicas": new_replicas}}
            )
            
            # 等待扩缩容完成
            self._wait_for_scaling_complete(new_replicas)
            
            return True
        except Exception as e:
            self.logger.error(f"Failed to update replicas: {e}")
            return False
    
    def _wait_for_scaling_complete(self, expected_replicas: int, timeout: int = 300):
        """等待扩缩容完成"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            current_replicas = self._get_current_replicas()
            ready_replicas = self._get_ready_replicas()
            
            if current_replicas == expected_replicas and ready_replicas == expected_replicas:
                self.logger.info(f"Scaling completed: {expected_replicas} replicas ready")
                return True
            
            self.logger.info(f"Waiting for scaling: {ready_replicas}/{expected_replicas} ready")
            time.sleep(10)
        
        self.logger.error(f"Scaling timeout after {timeout} seconds")
        return False
    
    def _get_ready_replicas(self) -> int:
        """获取就绪副本数"""
        try:
            pods = self.core_v1.list_namespaced_pod(
                namespace="picotron",
                label_selector="app=picotron-worker"
            )
            
            ready_count = 0
            for pod in pods.items:
                if pod.status.phase == "Running":
                    ready_count += 1
            
            return ready_count
        except Exception as e:
            self.logger.error(f"Failed to get ready replicas: {e}")
            return 0
    
    def run_scaling_loop(self, check_interval: int = 30):
        """运行扩缩容循环"""
        self.logger.info("Starting auto-scaling loop")
        
        while True:
            try:
                # 检查扩缩容条件
                direction = self.check_scaling_conditions()
                
                if direction:
                    # 执行扩缩容
                    self.scale(direction)
                
                # 等待下一次检查
                time.sleep(check_interval)
                
            except KeyboardInterrupt:
                self.logger.info("Auto-scaling loop stopped by user")
                break
            except Exception as e:
                self.logger.error(f"Error in scaling loop: {e}")
                time.sleep(check_interval)

# 配置示例
def create_scaling_config() -> ScalingConfig:
    """创建扩缩容配置"""
    metrics = [
        ScalingMetric(
            name="avg_cpu_usage",
            value=0.0,
            threshold=80.0,
            comparison="gt"
        ),
        ScalingMetric(
            name="avg_memory_usage",
            value=0.0,
            threshold=85.0,
            comparison="gt"
        ),
        ScalingMetric(
            name="avg_gpu_utilization",
            value=0.0,
            threshold=75.0,
            comparison="gt"
        ),
        ScalingMetric(
            name="training_queue_length",
            value=0.0,
            threshold=10.0,
            comparison="gt"
        )
    ]
    
    return ScalingConfig(
        min_replicas=3,
        max_replicas=10,
        scale_up_threshold=0.8,
        scale_down_threshold=0.3,
        cooldown_period=300,  # 5分钟
        metrics=metrics
    )

if __name__ == "__main__":
    # 配置日志
    logging.basicConfig(level=logging.INFO)
    
    # 创建自动扩缩容器
    config = create_scaling_config()
    auto_scaler = AutoScaler(config)
    
    # 运行扩缩容循环
    auto_scaler.run_scaling_loop()
```

## 5. 灾难恢复

### 5.1 备份策略

#### 5.1.1 数据备份

**实现方案：**
```python
# picotron/ops/backup.py
import os
import shutil
import tarfile
import hashlib
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import logging
import boto3
from botocore.exceptions import ClientError

class BackupManager:
    """备份管理器"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # 初始化存储客户端
        self.storage_client = self._init_storage_client()
        
        # 备份元数据
        self.backup_metadata = {}
        self.metadata_file = "backup_metadata.json"
        
        # 加载备份元数据
        self._load_backup_metadata()
    
    def _init_storage_client(self):
        """初始化存储客户端"""
        storage_type = self.config.get('storage_type', 'local')
        
        if storage_type == 's3':
            try:
                return boto3.client(
                    's3',
                    aws_access_key_id=self.config.get('aws_access_key_id'),
                    aws_secret_access_key=self.config.get('aws_secret_access_key'),
                    region_name=self.config.get('region', 'us-west-2')
                )
            except Exception as e:
                self.logger.error(f"Failed to initialize S3 client: {e}")
                return None
        
        return None
    
    def _load_backup_metadata(self):
        """加载备份元数据"""
        try:
            with open(self.metadata_file, 'r') as f:
                self.backup_metadata = json.load(f)
        except FileNotFoundError:
            self.backup_metadata = {
                'backups': [],
                'last_backup': None,
                'backup_schedule': self.config.get('backup_schedule', {})
            }
    
    def _save_backup_metadata(self):
        """保存备份元数据"""
        with open(self.metadata_file, 'w') as f:
            json.dump(self.backup_metadata, f, indent=2)
    
    def create_backup(self, backup_type: str = 'full') -> Dict:
        """创建备份"""
        backup_id = f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        backup_dir = f"/tmp/backups/{backup_id}"
        
        try:
            # 创建备份目录
            os.makedirs(backup_dir, exist_ok=True)
            
            # 备份数据
            backup_data = {}
            
            if backup_type == 'full':
                backup_data = self._backup_full(backup_dir)
            elif backup_type == 'incremental':
                backup_data = self._backup_incremental(backup_dir)
            elif backup_type == 'checkpoint':
                backup_data = self._backup_checkpoints(backup_dir)
            
            # 创建备份包
            backup_file = self._create_backup_package(backup_dir, backup_id)
            
            # 计算校验和
            checksum = self._calculate_checksum(backup_file)
            
            # 上传备份
            backup_url = self._upload_backup(backup_file, backup_id)
            
            # 更新元数据
            backup_info = {
                'backup_id': backup_id,
                'backup_type': backup_type,
                'timestamp': datetime.now().isoformat(),
                'size': os.path.getsize(backup_file),
                'checksum': checksum,
                'location': backup_url,
                'data': backup_data
            }
            
            self.backup_metadata['backups'].append(backup_info)
            self.backup_metadata['last_backup'] = datetime.now().isoformat()
            self._save_backup_metadata()
            
            # 清理临时文件
            shutil.rmtree(backup_dir)
            os.remove(backup_file)
            
            self.logger.info(f"Backup created successfully: {backup_id}")
            return backup_info
            
        except Exception as e:
            self.logger.error(f"Failed to create backup: {e}")
            # 清理临时文件
            if os.path.exists(backup_dir):
                shutil.rmtree(backup_dir)
            raise
    
    def _backup_full(self, backup_dir: str) -> Dict:
        """完整备份"""
        backup_data = {
            'checkpoints': self._backup_checkpoints(backup_dir),
            'logs': self._backup_logs(backup_dir),
            'config': self._backup_config(backup_dir),
            'models': self._backup_models(backup_dir)
        }
        
        return backup_data
    
    def _backup_incremental(self, backup_dir: str) -> Dict:
        """增量备份"""
        backup_data = {
            'checkpoints': self._backup_checkpoints(backup_dir, incremental=True),
            'logs': self._backup_logs(backup_dir, incremental=True),
            'changed_files': self._get_changed_files()
        }
        
        return backup_data
    
    def _backup_checkpoints(self, backup_dir: str, incremental: bool = False) -> Dict:
        """备份检查点"""
        checkpoint_dir = os.path.join(backup_dir, 'checkpoints')
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        checkpoints = {}
        checkpoint_source = self.config.get('checkpoint_dir', '/app/checkpoints')
        
        if os.path.exists(checkpoint_source):
            for checkpoint_name in os.listdir(checkpoint_source):
                source_path = os.path.join(checkpoint_source, checkpoint_name)
                dest_path = os.path.join(checkpoint_dir, checkpoint_name)
                
                if incremental:
                    # 只备份最近修改的检查点
                    mod_time = os.path.getmtime(source_path)
                    if mod_time > self._get_last_backup_time():
                        shutil.copy2(source_path, dest_path)
                        checkpoints[checkpoint_name] = {
                            'size': os.path.getsize(dest_path),
                            'modified': datetime.fromtimestamp(mod_time).isoformat()
                        }
                else:
                    shutil.copy2(source_path, dest_path)
                    checkpoints[checkpoint_name] = {
                        'size': os.path.getsize(dest_path),
                        'modified': datetime.fromtimestamp(os.path.getmtime(source_path)).isoformat()
                    }
        
        return checkpoints
    
    def _backup_logs(self, backup_dir: str, incremental: bool = False) -> Dict:
        """备份日志"""
        log_dir = os.path.join(backup_dir, 'logs')
        os.makedirs(log_dir, exist_ok=True)
        
        logs = {}
        log_source = self.config.get('log_dir', '/app/logs')
        
        if os.path.exists(log_source):
            for log_name in os.listdir(log_source):
                if log_name.endswith('.log'):
                    source_path = os.path.join(log_source, log_name)
                    dest_path = os.path.join(log_dir, log_name)
                    
                    if incremental:
                        # 只备份最近的日志
                        mod_time = os.path.getmtime(source_path)
                        if mod_time > self._get_last_backup_time():
                            shutil.copy2(source_path, dest_path)
                            logs[log_name] = {
                                'size': os.path.getsize(dest_path),
                                'modified': datetime.fromtimestamp(mod_time).isoformat()
                            }
                    else:
                        shutil.copy2(source_path, dest_path)
                        logs[log_name] = {
                            'size': os.path.getsize(dest_path),
                            'modified': datetime.fromtimestamp(os.path.getmtime(source_path)).isoformat()
                        }
        
        return logs
    
    def _backup_config(self, backup_dir: str) -> Dict:
        """备份配置"""
        config_dir = os.path.join(backup_dir, 'config')
        os.makedirs(config_dir, exist_ok=True)
        
        configs = {}
        config_files = [
            'config.yaml',
            'hyperparameters.json',
            'model_config.json'
        ]
        
        for config_file in config_files:
            source_path = os.path.join('/app', config_file)
            if os.path.exists(source_path):
                dest_path = os.path.join(config_dir, config_file)
                shutil.copy2(source_path, dest_path)
                configs[config_file] = {
                    'size': os.path.getsize(dest_path),
                    'modified': datetime.fromtimestamp(os.path.getmtime(source_path)).isoformat()
                }
        
        return configs
    
    def _backup_models(self, backup_dir: str) -> Dict:
        """备份模型"""
        model_dir = os.path.join(backup_dir, 'models')
        os.makedirs(model_dir, exist_ok=True)
        
        models = {}
        model_source = self.config.get('model_dir', '/app/models')
        
        if os.path.exists(model_source):
            for model_name in os.listdir(model_source):
                source_path = os.path.join(model_source, model_name)
                dest_path = os.path.join(model_dir, model_name)
                
                # 创建模型备份包
                model_backup_file = self._create_model_backup(source_path, dest_path)
                
                models[model_name] = {
                    'backup_file': model_backup_file,
                    'size': os.path.getsize(model_backup_file),
                    'modified': datetime.fromtimestamp(os.path.getmtime(source_path)).isoformat()
                }
        
        return models
    
    def _create_model_backup(self, source_path: str, dest_path: str) -> str:
        """创建模型备份包"""
        backup_file = f"{dest_path}.tar.gz"
        
        with tarfile.open(backup_file, 'w:gz') as tar:
            tar.add(source_path, arcname=os.path.basename(source_path))
        
        return backup_file
    
    def _create_backup_package(self, backup_dir: str, backup_id: str) -> str:
        """创建备份包"""
        backup_file = f"/tmp/{backup_id}.tar.gz"
        
        with tarfile.open(backup_file, 'w:gz') as tar:
            tar.add(backup_dir, arcname=backup_id)
        
        return backup_file
    
    def _calculate_checksum(self, file_path: str) -> str:
        """计算文件校验和"""
        sha256_hash = hashlib.sha256()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                sha256_hash.update(chunk)
        return sha256_hash.hexdigest()
    
    def _upload_backup(self, backup_file: str, backup_id: str) -> str:
        """上传备份"""
        storage_type = self.config.get('storage_type', 'local')
        
        if storage_type == 's3' and self.storage_client:
            return self._upload_to_s3(backup_file, backup_id)
        else:
            return self._upload_to_local(backup_file, backup_id)
    
    def _upload_to_s3(self, backup_file: str, backup_id: str) -> str:
        """上传到S3"""
        bucket_name = self.config.get('s3_bucket', 'picotron-backups')
        object_key = f"backups/{backup_id}.tar.gz"
        
        try:
            self.storage_client.upload_file(
                backup_file,
                bucket_name,
                object_key
            )
            
            return f"s3://{bucket_name}/{object_key}"
        except ClientError as e:
            self.logger.error(f"Failed to upload to S3: {e}")
            raise
    
    def _upload_to_local(self, backup_file: str, backup_id: str) -> str:
        """上传到本地"""
        backup_dir = self.config.get('local_backup_dir', '/app/backups')
        os.makedirs(backup_dir, exist_ok=True)
        
        dest_path = os.path.join(backup_dir, f"{backup_id}.tar.gz")
        shutil.copy2(backup_file, dest_path)
        
        return dest_path
    
    def restore_backup(self, backup_id: str, restore_dir: str = '/app') -> bool:
        """恢复备份"""
        try:
            # 查找备份信息
            backup_info = self._find_backup_info(backup_id)
            if not backup_info:
                self.logger.error(f"Backup not found: {backup_id}")
                return False
            
            # 下载备份
            backup_file = self._download_backup(backup_info)
            
            # 解压备份
            self._extract_backup(backup_file, restore_dir)
            
            # 验证恢复
            if self._verify_restore(backup_info, restore_dir):
                self.logger.info(f"Backup restored successfully: {backup_id}")
                return True
            else:
                self.logger.error(f"Backup verification failed: {backup_id}")
                return False
                
        except Exception as e:
            self.logger.error(f"Failed to restore backup: {e}")
            return False
    
    def _find_backup_info(self, backup_id: str) -> Optional[Dict]:
        """查找备份信息"""
        for backup in self.backup_metadata['backups']:
            if backup['backup_id'] == backup_id:
                return backup
        return None
    
    def _download_backup(self, backup_info: Dict) -> str:
        """下载备份"""
        backup_location = backup_info['location']
        backup_file = f"/tmp/{backup_info['backup_id']}.tar.gz"
        
        if backup_location.startswith('s3://'):
            # 从S3下载
            bucket_name, object_key = backup_location[5:].split('/', 1)
            self.storage_client.download_file(bucket_name, object_key, backup_file)
        else:
            # 从本地复制
            shutil.copy2(backup_location, backup_file)
        
        return backup_file
    
    def _extract_backup(self, backup_file: str, restore_dir: str):
        """解压备份"""
        with tarfile.open(backup_file, 'r:gz') as tar:
            tar.extractall(path=restore_dir)
    
    def _verify_restore(self, backup_info: Dict, restore_dir: str) -> bool:
        """验证恢复"""
        # 验证文件存在性
        for data_type, data_info in backup_info['data'].items():
            if data_type == 'checkpoints':
                for checkpoint_name in data_info.keys():
                    checkpoint_path = os.path.join(restore_dir, 'checkpoints', checkpoint_name)
                    if not os.path.exists(checkpoint_path):
                        return False
            elif data_type == 'logs':
                for log_name in data_info.keys():
                    log_path = os.path.join(restore_dir, 'logs', log_name)
                    if not os.path.exists(log_path):
                        return False
        
        # 验证校验和
        # 这里可以添加更详细的验证逻辑
        
        return True
    
    def cleanup_old_backups(self, retention_days: int = 30):
        """清理旧备份"""
        cutoff_date = datetime.now() - timedelta(days=retention_days)
        
        backups_to_remove = []
        for backup in self.backup_metadata['backups']:
            backup_date = datetime.fromisoformat(backup['timestamp'])
            if backup_date < cutoff_date:
                backups_to_remove.append(backup)
        
        for backup in backups_to_remove:
            self._delete_backup(backup)
            self.backup_metadata['backups'].remove(backup)
        
        self._save_backup_metadata()
        
        self.logger.info(f"Cleaned up {len(backups_to_remove)} old backups")
    
    def _delete_backup(self, backup_info: Dict):
        """删除备份"""
        backup_location = backup_info['location']
        
        if backup_location.startswith('s3://'):
            # 从S3删除
            bucket_name, object_key = backup_location[5:].split('/', 1)
            try:
                self.storage_client.delete_object(Bucket=bucket_name, Key=object_key)
            except ClientError as e:
                self.logger.error(f"Failed to delete from S3: {e}")
        else:
            # 从本地删除
            try:
                os.remove(backup_location)
            except OSError as e:
                self.logger.error(f"Failed to delete local file: {e}")
    
    def _get_last_backup_time(self) -> float:
        """获取上次备份时间"""
        if self.backup_metadata['last_backup']:
            return datetime.fromisoformat(self.backup_metadata['last_backup']).timestamp()
        return 0
    
    def _get_changed_files(self) -> List[str]:
        """获取修改的文件"""
        # 这里可以实现更复杂的文件变更检测逻辑
        return []
    
    def schedule_backups(self):
        """调度备份"""
        schedule = self.config.get('backup_schedule', {})
        
        while True:
            try:
                current_time = datetime.now()
                
                # 检查是否需要执行备份
                if self._should_run_backup(current_time, schedule):
                    self.create_backup('full')
                
                # 等待下一次检查
                time.sleep(3600)  # 每小时检查一次
                
            except KeyboardInterrupt:
                self.logger.info("Backup scheduler stopped by user")
                break
            except Exception as e:
                self.logger.error(f"Error in backup scheduler: {e}")
                time.sleep(3600)
    
    def _should_run_backup(self, current_time: datetime, schedule: Dict) -> bool:
        """判断是否应该运行备份"""
        # 检查每日备份
        if schedule.get('daily', {}).get('enabled', False):
            daily_time = schedule['daily'].get('time', '02:00')
            backup_time = datetime.strptime(daily_time, '%H:%M').time()
            
            if (current_time.time() >= backup_time and 
                current_time.time() < backup_time.replace(second=59, microsecond=999999))):
                # 检查是否今天已经备份过
                last_backup = self.backup_metadata.get('last_backup')
                if last_backup:
                    last_backup_date = datetime.fromisoformat(last_backup).date()
                    if last_backup_date == current_time.date():
                        return False
                
                return True
        
        # 检查每周备份
        if schedule.get('weekly', {}).get('enabled', False):
            weekly_day = schedule['weekly'].get('day', 0)  # 0=Monday
            weekly_time = schedule['weekly'].get('time', '02:00')
            
            if (current_time.weekday() == weekly_day and
                current_time.time() >= datetime.strptime(weekly_time, '%H:%M').time()):
                return True
        
        return False

# 备份配置示例
BACKUP_CONFIG = {
    'storage_type': 's3',
    's3_bucket': 'picotron-backups',
    'aws_access_key_id': 'your-access-key',
    'aws_secret_access_key': 'your-secret-key',
    'region': 'us-west-2',
    'checkpoint_dir': '/app/checkpoints',
    'log_dir': '/app/logs',
    'model_dir': '/app/models',
    'local_backup_dir': '/app/backups',
    'backup_schedule': {
        'daily': {
            'enabled': True,
            'time': '02:00'
        },
        'weekly': {
            'enabled': True,
            'day': 0,  # Monday
            'time': '02:00'
        }
    }
}

if __name__ == "__main__":
    # 配置日志
    logging.basicConfig(level=logging.INFO)
    
    # 创建备份管理器
    backup_manager = BackupManager(BACKUP_CONFIG)
    
    # 运行备份调度器
    backup_manager.schedule_backups()
```

## 6. 总结

部署和运维优化是确保Picotron项目稳定运行的关键。通过本文档提出的部署架构、监控系统和自动化运维工具，可以显著提高项目的可用性和可维护性。

### 6.1 部署优化成果

**预期改进效果：**
- 部署时间减少80%
- 故障恢复时间减少90%
- 运维工作量减少70%
- 系统可用性达到99.9%

### 6.2 长期运维策略

**持续改进：**
1. **监控体系完善**：持续优化监控指标和告警规则
2. **自动化程度提升**：增加更多自动化运维功能
3. **灾难恢复演练**：定期进行灾难恢复演练
4. **性能优化**：持续优化系统性能

### 6.3 成本优化

**成本控制：**
- 自动扩缩容降低资源成本
- 备份策略优化存储成本
- 监控数据管理降低日志成本
- 自动化运维降低人力成本

通过这些部署和运维优化措施，Picotron将成为一个高可用、高性能、易维护的分布式训练框架，为用户提供稳定可靠的服务。